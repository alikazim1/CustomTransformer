1) Model.py:
 GPT Language Model
This repository contains a PyTorch implementation of a simplified GPT (Generative Pre-trained Transformer) language model. The model can be trained on a text corpus and generate new text based on the learned patterns.

Overview
This implementation includes:

Data preprocessing to encode and decode text.
A Transformer-based model architecture with multi-head self-attention.
Training and evaluation loops to optimize the model.
Functions for text generation.
Requirements
Python 3.x
PyTorch
NumPy



2) Bigram.py:
Bigram GPT Language Model
This repository contains an implementation of a Bigram-based GPT language model using PyTorch. The model generates text by learning from an input corpus, leveraging transformer architecture components like multi-head attention and feed-forward networks.

Table of Contents
Bigram GPT Language Model
Table of Contents
Features
Installation
Usage
Training
Generating Text
Code Overview
Hyperparameters
References
License
Features
Transformer-based architecture with multi-head self-attention.
Supports custom vocabulary size.
Can generate text based on learned patterns from an input text file.
Installation
To install the required dependencies, use pip:


![andrea-de-santis-zwd435-ewb4-unsplash](https://github.com/alikazim1/CustomTransformer/assets/115345833/f66aa8a2-d22f-470f-97ee-52feb5aec98a)
