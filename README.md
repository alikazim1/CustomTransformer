1) Model.py:
GPT Language Model
This repository contains a PyTorch implementation of a simplified GPT (Generative Pre-trained Transformer) language model. The model can be trained on a text corpus and     generate new text based on the learned patterns.
This implementation includes: Data preprocessing to encode and decode text.
A Transformer-based model architecture with multi-head self-attention.
Training and evaluation loops to optimize the model.
Functions for text generation.
Requirements
Python 3.x
PyTorch
NumPy



2) Bigram.py:
 Bigram GPT Language Model
 This repository contains an implementation of a Bigram-based GPT language model using PyTorch. The model generates text by learning from an input corpus, leveraging      
 transformer architecture components like multi-head attention and feed-forward networks.
 Table of Contents
 Bigram GPT Language Model
 Table of Contents
 Features
 Installation
 Usage
 Training
 Generating Text
 Code Overview
 Hyperparameters
 References
 License
 Features
 Transformer-based architecture with multi-head self-attention.
 Supports custom vocabulary size.
 Can generate text based on learned patterns from an input text file.
 Installation
 To install the required dependencies, use pip:


![Screenshot 2024-05-21 130404](https://github.com/alikazim1/CustomTransformer/assets/115345833/07ab1947-07dc-4cf7-9ce1-eb41b28c37ca)
